{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa1XjcaP0Hl8KsiW7sk4yS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jijnash22/multimodal-parkinsons-ai/blob/main/notebooks/04_multimodal_fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Pwd2052zHafH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be87f292-a6c0-4c96-d754-5fa9425c9a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/IIIT_Nagpur_PD_Project\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load embeddings\n",
        "voice_emb = np.load(BASE + \"/features/voice_embeddings.npy\")\n",
        "face_emb = np.load(BASE + \"/features/facial_embeddings.npy\")\n",
        "\n",
        "# Load models\n",
        "voice_model = load_model(BASE + \"/models/voice_model_32D.keras\")\n",
        "face_model  = load_model(BASE + \"/models/face_pd_classifier.keras\")\n",
        "\n",
        "# Predict PD scores\n",
        "PDv = voice_model.predict(voice_emb).flatten()\n",
        "PDf = face_model.predict(face_emb).flatten()\n",
        "\n",
        "print(\"Voice:\", len(PDv))\n",
        "print(\"Face :\", len(PDf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zEW3eWSagVj",
        "outputId": "312af7da-4c0f-44f0-8af0-cd5a8dc7db99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fd3c47cf4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fd3c47cf4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Voice: 81\n",
            "Face : 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "PDv = scaler.fit_transform(PDv.reshape(-1,1)).flatten()\n",
        "PDf = scaler.fit_transform(PDf.reshape(-1,1)).flatten()\n",
        "\n"
      ],
      "metadata": {
        "id": "4OvmWT7aIVVw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PD_hand = pd.read_csv(BASE + \"/features/hand_pd_scores.csv\")[\"PD_score\"].values\n",
        "PD_hand\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISEpI21xPD_x",
        "outputId": "58cde846-50ac-4b05-d79f-0024bf7b5a7f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.52072478, 0.35071088, 0.3687345 , 1.        ,\n",
              "       0.6197075 , 0.298603  , 0.9534077 ])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "PDv = scaler.fit_transform(PDv.reshape(-1,1)).flatten()\n",
        "PDf = scaler.fit_transform(PDf.reshape(-1,1)).flatten()\n",
        "\n"
      ],
      "metadata": {
        "id": "zzYD1rmoPMTN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "import numpy as np\n",
        "\n",
        "hc_files = glob.glob(BASE + \"/data/voice_dataset/HC/HC_AH/*.wav\")\n",
        "pd_files = glob.glob(BASE + \"/data/voice_dataset/PD/PD_AH/*.wav\")\n",
        "\n",
        "y_voice = np.array([0]*len(hc_files) + [1]*len(pd_files))\n",
        "\n",
        "print(\"Voice labels:\", y_voice.shape, \" PD:\", y_voice.sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcnNilVOPOjn",
        "outputId": "a748bc42-bb34-46bd-febf-056e6307c9bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voice labels: (81,)  PD: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_face = np.array([0]*20 + [1]*20)\n",
        "print(\"Face labels:\", y_face.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZgrGSA9P967",
        "outputId": "9d5b5280-d3fe-4ddf-ed5d-fb2ef6e1a266"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Face labels: (40,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hand = np.array([0,0,0,0,1,0,0,1])\n",
        "print(\"Hand labels:\", y_hand)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9n8bjOCQAHO",
        "outputId": "b773eeea-17b5-4829-e447-678ce4045a79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hand labels: [0 0 0 0 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find \"/content/drive/MyDrive\" -type f | grep hand_pd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiHKokVWQulP",
        "outputId": "2c7bbafe-b5e4-4a6b-9c28-0a5142b1dbcc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IIIT_Nagpur_PD_Project/features/hand_pd_scores.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PD_hand = pd.read_csv(\"/content/drive/MyDrive/IIIT_Nagpur_PD_Project/features/hand_pd_scores.csv\")[\"PD_score\"].values\n",
        "PD_hand\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5D9xFeWRB2e",
        "outputId": "42c42117-ca8f-40c9-bff0-2691aa571ef9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.52072478, 0.35071088, 0.3687345 , 1.        ,\n",
              "       0.6197075 , 0.298603  , 0.9534077 ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "hc_files = glob.glob(BASE + \"/data/voice_dataset/HC/HC_AH/*.wav\")\n",
        "pd_files = glob.glob(BASE + \"/data/voice_dataset/PD/PD_AH/*.wav\")\n",
        "\n",
        "y_voice = np.array([0]*len(hc_files) + [1]*len(pd_files))\n",
        "len(hc_files), len(pd_files), len(y_voice), len(PDv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrVhP2EoRSFA",
        "outputId": "ba741e26-b60f-456f-c1e0-f21e12ec0ef5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 40, 81, 81)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "results = {}\n",
        "\n",
        "results[\"Voice only\"] = roc_auc_score(y_voice, PDv)\n",
        "results[\"Face only\"]  = roc_auc_score(y_face, PDf)\n",
        "results[\"Hand only\"]  = roc_auc_score(y_hand, PD_hand)\n",
        "\n",
        "results[\"Voice + Face\"] = roc_auc_score(y_face, (PDv[:40] + PDf) / 2)\n",
        "results[\"Voice + Hand\"] = roc_auc_score(y_hand, (PDv[:8] + PD_hand) / 2)\n",
        "results[\"Face + Hand\"]  = roc_auc_score(y_hand, (PDf[:8] + PD_hand) / 2)\n",
        "\n",
        "results[\"All 3\"] = roc_auc_score(\n",
        "    y_hand,\n",
        "    (PDv[:8] + PDf[:8] + PD_hand) / 3\n",
        ")\n",
        "\n",
        "results\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMhfGGppRaDl",
        "outputId": "86b53123-9222-4c77-a55a-8503af072b3e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Voice only': np.float64(0.901829268292683),\n",
              " 'Face only': np.float64(0.5),\n",
              " 'Hand only': np.float64(1.0),\n",
              " 'Voice + Face': np.float64(0.6950000000000001),\n",
              " 'Voice + Hand': np.float64(1.0),\n",
              " 'Face + Hand': np.float64(1.0),\n",
              " 'All 3': np.float64(1.0)}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJBuNc2XRbuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}